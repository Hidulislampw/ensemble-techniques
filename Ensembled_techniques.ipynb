{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcg9y6Eo-uK3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions"
      ],
      "metadata": {
        "id": "yNRs8zSR-2ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems\n",
        "Ans 1. Yes, bagging (Bootstrap Aggregating) can definitely be used for regression problems. In regression, the goal is to predict a continuous output variable, and bagging helps improve the accuracy and robustness of these predictions by combining the outputs of multiple base regressors. It works by creating multiple subsets of the original training dataset through bootstrapping (random sampling with replacement), training a separate regression model on each subset, and then aggregating the individual predictions — usually by averaging them — to obtain the final output. This process reduces variance, helps prevent overfitting, and can significantly enhance the performance of unstable models like decision trees. Therefore, bagging is a powerful ensemble method suitable not only for classification but also for regression tasks.\n"
      ],
      "metadata": {
        "id": "14PT51uK-5f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is the difference between multiple model training and single model training\n",
        "Ans 2. The primary difference between multiple model training and single model training lies in the number of models being developed and the objectives they serve. In **single model training**, only one machine learning model is trained on the entire dataset to perform a specific task, such as classification or regression. This approach is straightforward, easier to manage, and suitable when the problem is well-defined and the dataset is homogeneous. On the other hand, **multiple model training** involves training several models either independently or as part of an ensemble approach. This can be done to compare performance across different algorithms, improve overall prediction accuracy through methods like bagging, boosting, or stacking, or handle complex tasks such as multi-output problems or domain adaptation. Multiple model training often yields better generalization and robustness, but it is more computationally intensive and complex to implement and maintain compared to single model training.\n"
      ],
      "metadata": {
        "id": "wTBtyHQj_h5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest\n",
        "Ans 3. Feature randomness in Random Forest refers to the technique of introducing randomness in the selection of features at each split during the construction of decision trees. Unlike a traditional decision tree, where the best feature is chosen from the entire set of input features to split the data at each node, Random Forest selects a random subset of features at each node and then chooses the best feature among them for splitting. This random selection helps to reduce the correlation between individual trees in the ensemble, making the overall model more robust and less prone to overfitting. By ensuring that not all trees use the same dominant features, feature randomness promotes diversity among the trees, which improves the generalization performance of the Random Forest algorithm. This is a key reason why Random Forest often performs better than individual decision trees in predictive accuracy and model stability.\n"
      ],
      "metadata": {
        "id": "2U1bYZbd_v4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is OOB (Out-of-Bag) Score\n",
        "Ans 4. The Out-of-Bag (OOB) score is a performance metric used in ensemble learning methods, particularly in Random Forest algorithms, to estimate the model’s accuracy without the need for a separate validation dataset. In Random Forest, each decision tree is trained on a random subset of the data selected with replacement (bootstrapping), meaning some data points are left out during the training of each tree. These excluded data points are known as \"out-of-bag\" samples. The OOB score is calculated by testing each tree on its respective OOB samples and averaging the prediction accuracy across all trees. This provides an internal cross-validation method that helps assess the model's generalization performance, making it a useful and efficient tool, especially when data is limited.\n"
      ],
      "metadata": {
        "id": "iKjwR14dAAk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How can you measure the importance of features in a Random Forest model\n",
        "Ans 5. The importance of features in a Random Forest model can be measured using two primary methods: **Mean Decrease in Impurity (MDI)** and **Mean Decrease in Accuracy (MDA)**. MDI, also known as Gini importance, is calculated during the training process and is based on how much each feature decreases the impurity (such as Gini impurity or entropy) across all trees in the forest. Each time a feature is used to split a node, the resulting decrease in impurity is recorded, and the total decrease accumulated over all trees indicates the feature’s importance. On the other hand, MDA, or permutation importance, evaluates the impact of each feature on the model’s predictive performance by randomly shuffling the values of a feature and measuring the change in model accuracy. A significant drop in accuracy indicates a high importance of that feature. Both methods provide insights into how the model uses the features, although permutation importance is generally considered more reliable, especially when dealing with correlated features. Tools like scikit-learn offer built-in functions to compute and visualize these feature importances.\n"
      ],
      "metadata": {
        "id": "g5S70msnBOVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier\n",
        "Ans 6. The Bagging Classifier, short for **Bootstrap Aggregating**, is an ensemble machine learning technique that improves the accuracy and stability of models by combining the predictions of multiple base learners, typically decision trees. The core idea behind bagging is to create several subsets of the original training dataset using **random sampling with replacement** (bootstrap sampling), so each model is trained on a different subset. These individual models are trained independently and may perform differently due to the variations in data. When it comes to making predictions, the Bagging Classifier aggregates the outputs of all base learners — using **majority voting for classification** or **averaging for regression** — to produce a final prediction. This approach helps to reduce **variance**, minimize the risk of overfitting, and increase the robustness of the model, especially when using high-variance, low-bias algorithms like decision trees. A popular example of a Bagging Classifier is the **Random Forest**, which adds an additional layer of randomness by also selecting random subsets of features at each split in the decision trees.\n"
      ],
      "metadata": {
        "id": "8jX2omHcBjjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  How do you evaluate a Bagging Classifier’s performance\n",
        "Ans 7. To evaluate the performance of a **Bagging Classifier**, several key metrics and validation strategies are used. Firstly, you assess its accuracy by using a **confusion matrix** that gives insight into true positives, true negatives, false positives, and false negatives, which helps in calculating accuracy, precision, recall, and F1-score. These metrics are especially important when dealing with imbalanced datasets. Next, **cross-validation**—such as k-fold cross-validation—is commonly used to ensure the model's performance is not dependent on a particular train-test split and to check its generalizability. Additionally, **ROC-AUC score** is useful for evaluating the classifier’s ability to distinguish between classes, particularly in binary classification tasks. You can also compare the performance of the Bagging Classifier to its base estimator (e.g., decision tree) to verify whether bagging improved the performance by reducing variance. Lastly, evaluating metrics like **training time**, **prediction time**, and **overfitting tendencies** can provide a more comprehensive picture of the model’s efficiency and robustness.\n"
      ],
      "metadata": {
        "id": "U-dnQfqcBvTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does a Bagging Regressor work\n",
        "ANs 8. A Bagging Regressor works by combining the predictions of multiple individual regression models to improve overall accuracy and reduce overfitting. The term \"Bagging\" stands for *Bootstrap Aggregating*, which involves training each base estimator (often decision trees) on different random subsets of the training data created using bootstrapping (sampling with replacement). Each model independently learns from its subset, and during prediction, their outputs are averaged to produce a final result. This ensemble approach reduces variance, making the model more robust and stable compared to a single estimator, particularly in the presence of noise or small data fluctuations.\n"
      ],
      "metadata": {
        "id": "Dc5-B620B90D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques\n",
        "Ans 9. The main advantage of ensemble techniques lies in their ability to significantly improve predictive performance by combining multiple models to make a final decision. Unlike individual models that may suffer from high variance, bias, or overfitting, ensemble methods like bagging, boosting, and stacking leverage the strengths of diverse learners to reduce errors and enhance generalization. By aggregating the outcomes of several weak or strong learners, ensembles produce more robust, accurate, and stable predictions, making them highly effective for complex and high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "lPGirE8DCOPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods\n",
        "Ans 10. The main challenge of ensemble methods lies in balancing **model complexity and computational cost** while ensuring that the combined models offer meaningful improvements over individual models. Ensemble techniques, such as bagging, boosting, and stacking, rely on generating multiple base learners and aggregating their predictions to enhance performance. However, this often results in increased **training time, memory usage, and reduced interpretability**, making them less suitable for real-time or resource-constrained applications. Additionally, if the base models are not sufficiently diverse or if the ensemble is poorly constructed, the method may suffer from **overfitting** or provide only marginal gains. Therefore, designing effective ensembles requires careful selection of algorithms, hyperparameters, and validation strategies to truly benefit from their collective strength.\n"
      ],
      "metadata": {
        "id": "QPiJSu_GCaIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques\n",
        "ANs 11. The key idea behind ensemble techniques is to combine the predictions of multiple individual models to create a more robust, accurate, and generalizable predictive model. Instead of relying on a single model, ensemble methods leverage the strengths and diversity of several models to reduce errors caused by bias, variance, or noise. By aggregating their outputs—through techniques like bagging, boosting, or stacking—ensemble approaches often outperform individual models, especially in complex or high-dimensional datasets. This collective decision-making strategy enhances overall performance and stability, making ensemble techniques a powerful tool in both classification and regression tasks.\n"
      ],
      "metadata": {
        "id": "iQyQpef3DIWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier\n",
        "Ans 12. A **Random Forest Classifier** is an ensemble machine learning algorithm used for classification tasks, which operates by constructing multiple decision trees during training and outputting the class that is the mode (majority vote) of the classes predicted by individual trees. It combines the concept of \"bagging\" (bootstrap aggregating) with decision trees, where each tree is trained on a random subset of the data and features, ensuring diversity among the trees. This reduces the risk of overfitting and improves generalization compared to a single decision tree. Random Forest is known for its robustness, high accuracy, and ability to handle both numerical and categorical data, as well as its effectiveness in dealing with missing values and outliers.\n"
      ],
      "metadata": {
        "id": "Vh9Rsm1YDWLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What are the main types of ensemble techniques\n",
        "Ans13. Ensemble techniques in machine learning are methods that combine multiple models to improve overall performance, robustness, and generalization. The main types of ensemble techniques include **Bagging (Bootstrap Aggregating)**, **Boosting**, and **Stacking**. **Bagging** involves training multiple instances of the same model on different random subsets of the data (e.g., Random Forest), which helps reduce variance and prevent overfitting. **Boosting**, on the other hand, builds models sequentially, where each new model tries to correct the errors made by the previous ones (e.g., AdaBoost, Gradient Boosting, XGBoost), making it effective at reducing bias. **Stacking** combines predictions from several different types of models using a meta-learner, which learns to optimally combine the base models' outputs for improved accuracy. These ensemble techniques leverage the strengths of individual models and compensate for their weaknesses, leading to superior predictive performance compared to any single model.\n"
      ],
      "metadata": {
        "id": "ri4Eio5KDmqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning\n",
        "Ans 14. Ensemble learning in machine learning is a technique that combines the predictions of multiple models to produce a more accurate and robust output than any single model alone. By leveraging the strengths and compensating for the weaknesses of individual models, ensemble methods aim to reduce variance, bias, or improve predictions. There are various ensemble strategies such as bagging (e.g., Random Forest), boosting (e.g., AdaBoost, XGBoost), and stacking, each with a different approach to model combination. These methods are particularly useful in complex tasks where a single model might struggle to generalize well, making ensemble learning a powerful tool in both classification and regression problems.\n"
      ],
      "metadata": {
        "id": "Q6hCLSpcD66F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods\n",
        "Ans 15. Ensemble methods should be avoided in situations where model interpretability is crucial, such as in regulatory environments or clinical decision-making, where understanding the reasoning behind a prediction is essential. They are also less suitable when working with small datasets, as complex ensemble models can easily overfit the data, reducing generalizability. Additionally, if computational resources or time are limited, ensemble methods—especially those like boosting or bagging with many base learners—can be inefficient due to their high processing demands. Lastly, if a single, simpler model performs sufficiently well, adding ensemble complexity may yield only marginal improvements that do not justify the added effort and resource cost.\n"
      ],
      "metadata": {
        "id": "c88ai8rMEFvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting\n",
        "Ans 16. Bagging, or Bootstrap Aggregating, helps reduce overfitting by combining the predictions of multiple base models trained on different random subsets of the training data. Each model is trained on a bootstrap sample—created by sampling with replacement—which introduces diversity among the models. While a single model, especially a high-variance one like a decision tree, may overfit the training data, averaging the predictions (in regression) or taking a majority vote (in classification) across many such models reduces variance without significantly increasing bias. This ensemble approach smooths out the noise and idiosyncrasies of individual learners, resulting in a more generalized model that performs better on unseen data.\n"
      ],
      "metadata": {
        "id": "iub-6tnREU7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree\n",
        "Ans 17. Random Forest is generally considered better than a single Decision Tree because it combines the predictions of multiple decision trees to improve overall performance, reduce overfitting, and increase accuracy. While a single decision tree can be highly sensitive to noise and small changes in the training data—leading to high variance—Random Forest mitigates this by training multiple trees on different subsets of the data and features, then averaging their results (for regression) or using majority voting (for classification). This ensemble approach reduces the risk of overfitting and enhances generalization to unseen data, making Random Forest more robust, stable, and accurate than a single decision tree in most practical scenarios.\n"
      ],
      "metadata": {
        "id": "D6255sA3EefJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  What is the role of bootstrap sampling in Bagging\n",
        "Ans 18. Bootstrap sampling plays a crucial role in Bagging (Bootstrap Aggregating) by introducing randomness and diversity into the training process, which helps to reduce overfitting and improve model stability. In this approach, multiple subsets of the original training data are created by randomly sampling with replacement. This means that each new dataset may contain duplicate instances and omit others from the original data. These bootstrapped datasets are then used to train multiple base models (typically decision trees). The ensemble of these models combines their predictions—through averaging for regression or majority voting for classification—leading to a more robust and accurate final prediction. By leveraging bootstrap sampling, Bagging ensures that each base model learns slightly different patterns, reducing variance and enhancing generalization.\n"
      ],
      "metadata": {
        "id": "J0McOvH_En8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  What are some real-world applications of ensemble techniques\n",
        "Ans 19. Ensemble techniques are widely applied in real-world scenarios where high accuracy and robust performance are critical. In finance, they are used for credit scoring, stock market prediction, and fraud detection by combining multiple models to reduce risk and improve decision-making. In healthcare, ensemble methods aid in disease diagnosis and prognosis prediction by integrating various clinical models to enhance diagnostic accuracy. In e-commerce and marketing, ensemble techniques improve customer segmentation, recommendation systems, and churn prediction by aggregating diverse predictive models. Additionally, in cybersecurity, they help in detecting anomalies and preventing attacks by combining classifiers to filter threats more reliably. Overall, ensemble methods offer a powerful approach to tackling complex, data-driven problems across numerous industries.\n"
      ],
      "metadata": {
        "id": "a6jpnN6DEzP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  What is the difference between Bagging and Boosting?\n",
        "Ans 20. Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques used to improve the accuracy and robustness of machine learning models, but they differ fundamentally in how they build and combine multiple models. Bagging works by creating multiple independent models in parallel using different subsets of the training data (obtained through random sampling with replacement), and then aggregates their predictions, typically through majority voting (for classification) or averaging (for regression), to reduce variance and prevent overfitting. In contrast, Boosting builds models sequentially, where each new model tries to correct the errors made by the previous ones, focusing more on the misclassified or poorly predicted instances. This sequential approach reduces bias and can lead to higher accuracy, but it also makes Boosting more sensitive to noise and prone to overfitting if not carefully tuned.\n"
      ],
      "metadata": {
        "id": "4p8Lcg9mE-Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions"
      ],
      "metadata": {
        "id": "DxoBjfteFIe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21.  Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create base model (Decision Tree)\n",
        "base_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create Bagging Classifier\n",
        "bagging_model = BaggingClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "6neZxvLXFVz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "yCasIurDFq6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.  Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "pjENYx49F83V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24.  Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "XvzrgIW4GDlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25  Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset (not necessary for OOB, but useful for comparison)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest with OOB scoring enabled\n",
        "rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Print the Out-of-Bag score\n",
        "print(\"Random Forest OOB Score:\", rf_model.oob_score_)\n"
      ],
      "metadata": {
        "id": "FzWnbD93rdCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create base model (SVM)\n",
        "base_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier using SVM as base estimator\n",
        "bagging_model = BaggingClassifier(base_estimator=base_model, n_estimators=30, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier (SVM) Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "jKsmCqNZro2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Try different numbers of trees\n",
        "tree_counts = [10, 50, 100, 150, 200]\n",
        "print(\"Random Forest Accuracy with Different Numbers of Trees:\\n\")\n",
        "\n",
        "for n in tree_counts:\n",
        "    # Initialize Random Forest with n trees\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{n} Trees: Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "9tMEAMnhr3Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28.  Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create base model (Logistic Regression)\n",
        "base_model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "\n",
        "# Create Bagging Classifier using Logistic Regression\n",
        "bagging_model = BaggingClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for AUC calculation\n",
        "y_proba = bagging_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print AUC score\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"Bagging Classifier (Logistic Regression) AUC Score:\", auc)\n"
      ],
      "metadata": {
        "id": "k2oUQnMVsBts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29.  Train a Random Forest Regressor and analyze feature importance scores\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf_regressor.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance Score': importances\n",
        "}).sort_values(by='Importance Score', ascending=False)\n",
        "\n",
        "# Display the feature importance scores\n",
        "print(\"\\nFeature Importance Scores:\\n\")\n",
        "print(importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance Score'], color='skyblue')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.title(\"Feature Importances in Random Forest Regressor\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fGH1dwixsJ2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Bagging Classifier using Decision Trees\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# 2. Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
        "\n",
        "# Print comparison\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
        "print(f\"Random Forest Accuracy:     {rf_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "kacz2df_sceX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Random Forest Accuracy after Hyperparameter Tuning:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "id": "REVySWXiIUHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Estimators to try\n",
        "estimator_counts = [10, 50, 100, 150, 200]\n",
        "mse_scores = []\n",
        "\n",
        "print(\"Bagging Regressor MSE with Different Numbers of Base Estimators:\\n\")\n",
        "\n",
        "for n in estimator_counts:\n",
        "    model = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"{n} Estimators: MSE = {mse:.4f}\")\n",
        "\n",
        "# Plot the comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(estimator_counts, mse_scores, marker='o', color='teal')\n",
        "plt.title(\"Bagging Regressor Performance vs Number of Estimators\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CylUV8-VJh9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33 Train a Random Forest Classifier and analyze misclassified samples\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", round(accuracy, 4))\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]\n",
        "\n",
        "# Create DataFrame for misclassified samples\n",
        "misclassified_df = pd.DataFrame(X_test[misclassified_indices], columns=feature_names)\n",
        "misclassified_df['True Label'] = [target_names[i] for i in y_test[misclassified_indices]]\n",
        "misclassified_df['Predicted Label'] = [target_names[i] for i in y_pred[misclassified_indices]]\n",
        "\n",
        "print(\"\\nMisclassified Samples:\\n\")\n",
        "print(misclassified_df)\n"
      ],
      "metadata": {
        "id": "ut4hhLB9Jz3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Train a single Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_preds = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
        "\n",
        "# 2. Train a Bagging Classifier using Decision Trees\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Single Decision Tree Accuracy:\", round(dt_accuracy, 4))\n",
        "print(\"Bagging Classifier Accuracy:  \", round(bagging_accuracy, 4))\n"
      ],
      "metadata": {
        "id": "q3cEWv6RJ79K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Train a Random Forest Classifier and visualize the confusion matrix\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "class_names = data.target_names\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Random Forest Accuracy:\", round(accuracy, 4))\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K4aELJMyKGEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36.  Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, kernel='rbf', random_state=42))\n",
        "]\n",
        "\n",
        "# Define stacking classifier with Logistic Regression as final estimator\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train stacking classifier\n",
        "stacking_model.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_model.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Train and evaluate base models individually\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_accuracy = accuracy_score(y_test, dt_model.predict(X_test))\n",
        "\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_accuracy = accuracy_score(y_test, svm_model.predict(X_test))\n",
        "\n",
        "# Train and evaluate final estimator alone (Logistic Regression)\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "lr_accuracy = accuracy_score(y_test, lr_model.predict(X_test))\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy Comparison:\\n\")\n",
        "print(f\"Decision Tree Accuracy:       {dt_accuracy:.4f}\")\n",
        "print(f\"SVM Accuracy:                 {svm_accuracy:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NXj0kBqJLL77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37.  Train a Random Forest Classifier and print the top 5 most important features\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame of features and their importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\\n\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "id": "Db45Ooe1LaYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Bagging Classifier Performance Metrics:\\n\")\n",
        "print(f\"Precision:  {precision:.4f}\")\n",
        "print(f\"Recall:     {recall:.4f}\")\n",
        "print(f\"F1-Score:   {f1:.4f}\")\n",
        "\n",
        "# Optional: detailed class-wise report\n",
        "print(\"\\nDetailed Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "DGME6CqfLsci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Try different max_depth values\n",
        "depth_values = [1, 2, 3, 5, 7, 10, None]\n",
        "accuracies = []\n",
        "\n",
        "print(\"Random Forest Accuracy with Different max_depth Values:\\n\")\n",
        "\n",
        "for depth in depth_values:\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"max_depth = {depth}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "depth_labels = ['None' if d is None else str(d) for d in depth_values]\n",
        "plt.plot(depth_labels, accuracies, marker='o', linestyle='-', color='green')\n",
        "plt.title(\"Effect of max_depth on Random Forest Accuracy\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oxMV4y8eL34y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40.  Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "dt_base = DecisionTreeRegressor()\n",
        "knn_base = KNeighborsRegressor()\n",
        "\n",
        "# Create Bagging Regressors with different base estimators\n",
        "bagging_dt = BaggingRegressor(base_estimator=dt_base, n_estimators=50, random_state=42)\n",
        "bagging_knn = BaggingRegressor(base_estimator=knn_base, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train models\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "\n",
        "# Evaluate using MSE\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Print comparison\n",
        "print(\"Performance Comparison of Bagging Regressors:\\n\")\n",
        "print(f\"Bagging with Decision Tree Regressor:   MSE = {mse_dt:.4f}\")\n",
        "print(f\"Bagging with KNeighbors Regressor:      MSE = {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "yvpvAtNpMBuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_proba = rf_model.predict_proba(X_test)[:, 1]  # probabilities for class 1\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "AWLH-VzkMOig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Train a Bagging Classifier and evaluate its performance using cross-validation\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Define the Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform 5-fold cross-validation using accuracy\n",
        "cv_scores = cross_val_score(bagging_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Classifier Cross-Validation Accuracy Scores:\", np.round(cv_scores, 4))\n",
        "print(\"Mean Accuracy:\", round(np.mean(cv_scores), 4))\n",
        "print(\"Standard Deviation:\", round(np.std(cv_scores), 4))\n"
      ],
      "metadata": {
        "id": "gIGd9n1dMXAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43.  Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall, precision, color='blue', label=f'AP = {avg_precision:.4f}')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve - Random Forest Classifier\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-K6glhUJNWg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "]\n",
        "\n",
        "# Define stacking classifier with Logistic Regression as final estimator\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking classifier\n",
        "stacking_model.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_model.predict(X_test)\n",
        "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Train and evaluate Random Forest alone\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "acc_rf = accuracy_score(y_test, rf_model.predict(X_test))\n",
        "\n",
        "# Train and evaluate Logistic Regression alone\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "acc_lr = accuracy_score(y_test, lr_model.predict(X_test))\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Accuracy Comparison:\\n\")\n",
        "print(f\"Random Forest Accuracy:       {acc_rf:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {acc_stack:.4f}\")\n"
      ],
      "metadata": {
        "id": "Nr230wIyNlHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different max_samples percentages (proportion of data sampled for each estimator)\n",
        "sample_sizes = [0.4, 0.6, 0.8, 1.0]\n",
        "mse_scores = []\n",
        "\n",
        "print(\"Bagging Regressor Performance with Varying Bootstrap Sample Sizes:\\n\")\n",
        "\n",
        "for sample in sample_sizes:\n",
        "    model = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,\n",
        "        max_samples=sample,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"max_samples = {sample:.1f}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot([int(s*100) for s in sample_sizes], mse_scores, marker='o', color='navy')\n",
        "plt.title(\"Effect of Bootstrap Sample Size on Bagging Regressor Performance\")\n",
        "plt.xlabel(\"Bootstrap Sample Size (% of training data)\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ximdExCpNzMb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}